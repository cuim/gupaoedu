技术复习

## Zookeeper

### 用途

* 命名服务
  * 服务协议地址维护
* 配置管理
  * 分布式配置信息管理
* 集群管理
  * 负载均衡
  * Leader选举
* 分布式锁
  * 实现排他锁和共享锁

### 数据存储

采用key-value形式存储。

### 节点类型及特点

* 持久节点
  * 持久有序节点
* 临时节点
  * 临时有序节点(可以实现分布式锁)
* 有序节点

### 节点角色

* Leader
  * 负责事务性操作
* Fllower
  * 负责读取操作，参与事务性提案和Leader选举投票。
* Observer
  * 提供非事务操作，同步Master节点。不参与事务性提案和Leader选举投票。

### Zookeeper集群

由2n+1台服务器节点组成。要保证集群对外正常提供服务，必须保证多半服务器节点正常工作。

### ZAB协议

分布式协调服务协议。包括：

* 崩溃恢复

  一旦 Leader 节点崩溃， 或者由于网络问题导致 Leader 服务器失去了过半的Follower 节点的联系，就会进入崩溃恢复阶段。进行leader选举，需要解决如下问题：

  * 已经被处理的消息不能丢失

    当 leader 收到合法数量 follower 的 ACKs 后，就向各个 follower 广播 COMMIT 命令，同时也会在本地
    执行 COMMIT 并向连接的客户端返回「成功」。但是如果在各个 follower 在收到 COMMIT 命令前leader就挂了，导致剩下的服务器并没有执行都这条消息。

  * 被丢弃的消息不能再次出现

    当 leader 接收到消息请求生成 proposal 后就挂了，其他 follower 并没有收到此 proposal，因此经过恢复模式重新选了 leader 后，这条消息是被跳过的。

  Leader选举算法：

  * ZXID 也就是事务 id.

    所有的提议(proposal)都在被提出的时候加上了 zxid。实现中 zxid 是一个 64 位的数字，它高 32 位是 epoch(ZAB 协议通过 epoch 编号来区分 Leader 周期变化的策略)用来标识 leader 关系是否改变，每次一个 leader 被选出来，它都会有一个新的epoch=(原来的 epoch+1)，标识当前属于那个 leader 的统治时期。低 32 位用于递增计数。

  * Lead选举过程

    * 服务器启动时的 **leader** 选举

      投票用(maid，zxid，epoch)表示，集群的每个服务器收到 投票后，首先判断该投票的有效性，如检查是否是本 轮投票(epoch)、是否来自 LOOKING 状态的服务器。比较自身的投票和接受的投票。规则：

      * 优先检查 ZXID。ZXID 比较大的服务器优先作为Leader。

      * 如果 ZXID 相同，那么就比较 myid。myid 较大的服务器作为 Leader 服务器。

        如：对于 Server1 而言，它的投票是(1, 0)，接收 Server2的投票为(2, 0)，首先会比较两者的 ZXID，均为 0，再比较 myid，此时 Server2 的 myid 最大，于是更新自己的投票为(2, 0)，然后重新投票，对于 Server2 而言，它不需要更新自己的投票，只是再次向集群中所有机
        器发出上一次投票信息即可。统计投票。每次投票后，服务器都会统计投票信息， 判断是否已经有过半机器接受到相同的投票信息，对 于 Server1、Server2 而言，都统计出集群中已经有两 台机器接受了(2, 0)的投票信息，此时便认为已经选出 了 Leader。

    * 运行过程中的 **leader** 选举

      当集群中的 leader 服务器出现宕机或者不可用的情况时，进入新一轮的Leader 选举。

      基本与启动时的 **leader** 选举过程一致。

* 原子广播

  事务性提案提交。

  * 2pc协议

    要跨越多个分布式节点的时候，为了保持事务处理的 ACID特性，就需要引入一个“协调者”(TM)来统一调度所有分布式节点的执行逻辑，这些被调度的分布式节点被称为 AP。TM 负责调度 AP 的行为，并最终决定这些 AP 是否要把事务真正进行提交;因为整个事务是分为两个阶段提交，所以叫 2pc。

    * 阶段一:提交事务请求(投票)

      协调者询问事务，参与者确定可以100%执行事务进行反馈。过半反馈决定是否执行事务。

    * 阶段二:执行事务提交

      协调者会根据各参与者的反馈情况来决定最终是否可以进行事务提交操作。

  * 实现原理

    * leader 接收到消息请求后，将消息赋予一个全局唯一的64 位自增 id，叫:zxid，通过 zxid 的大小比较既可以实现因果有序这个特征。

    * leader 为每个 follower 准备了一个 FIFO 队列(通过 TCP协议来实现，以实现了全局有序这一个特点)将带有 zxid 的消息作为一个提案(proposal)分发给所有的 follower。

    * 当 follower 接收到 proposal，先把 proposal 写到磁盘， 写入成功以后再向 leader 回复一个 ack。

    * 当 leader 接收到合法数量(超过半数节点)的 ACK 后，leader 就会向这些 follower 发送 commit 命令，同时会在本地执行该消息。

    * 当 follower 收到消息的 commit 命令以后，会提交该消息。 

      ![image-20190224093524554](/Users/denny/Library/Application Support/typora-user-images/image-20190224093524554.png)

  ## Watch事件

  事件类型：

  ***None*** (-1)：客户端链接状态发生变化的时候，会收到 none 的事件。

  ***NodeCreated*** (1)：节点创建事件

  ***NodeDeleted*** (2)：节点删除事件

  ***NodeDataChanged*** (3)：节点数据改变事件

  ***NodeChildrenChanged*** (4)：子节点被创建、被删除会发生的事件

  |                               | zk-persis-mic( 监听事件）         | zk-persis-mic/child (监听事件) |
  | ----------------------------- | --------------------------------- | ------------------------------ |
  | create(/zk-persis-mic)        | NodeCreated(exists/getData)       | 无                             |
  | delete(/zk-persis-mic)        | NodeDeleted(exists/getData)       | 无                             |
  | setData(/zk-persis-mic))      | NodeDataChanged(exists / getData) | 无                             |
  | create(/zk-persis-mic/child)  | NodeChildrenChanged(getchild)     | NodeCreated                    |
  | delete(/zk-persis-mic/child)  | NodeChildrenChanged(getchild)     | NodeDeleted                    |
  | setData(/zk-persis-mic/child) | 无                                | NodeDataChanged                |

  ## Zookeeper分布锁实现

  使用临时有序节点如：/lock/lock-0000000000、/lock/lock-0000000001、/lock/lock-0000000002...。

  监听/lock的子节点删除消息。

  * 客户端连接zookeeper，并在/lock下创建临时的且有序的子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，[以此类推](https://www.baidu.com/s?wd=%E4%BB%A5%E6%AD%A4%E7%B1%BB%E6%8E%A8&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)。

  * 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁，否则监听/lock的子节点变更消息，获得子节点变更通知后重复此步骤直至获得锁；

  * 执行业务代码；

  * 完成业务流程后，删除对应的子节点释放锁。

  优化避免羊群效应

  监听自己之前一位的子节点删除消息。

  - 客户端连接zookeeper，并在/lock下创建临时的且有序的子节点，第一个客户端对应的子节为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推；
  - 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁，否则监听刚好在自己之前一位的子节点删除消息，获得子节点变更通知后重复此步骤直至获得锁；
  - 执行业务代码；
  - 完成业务流程后，删除对应的子节点释放锁。

  ### Zookeeper客户端

  * Curator

## Dubbo



### 架构图

![image-20190224113001779](/Users/denny/Library/Application Support/typora-user-images/image-20190224113001779.png)

### 案例

![image-20190224114554349](/Users/denny/Library/Application Support/typora-user-images/image-20190224114554349.png)

### Dubbo配置文件

### Dubbo服务端

配置功能：

* 提供方信息

  ```xml
  <dubbo:application name="dubbo-server" owner="denny"/>
  ```

* 注册中心（多个注册中心）

  支持Zookeeper、 Redis等

  ```xml
  <!-- 注册中心 zookeeper注册 file:指定协议地址缓存地址 -->
  	<dubbo:registry address="zookeeper://192.168.3.14:2181" file="／Users/denny/data/dubboCache"></dubbo:registry>
  
  ```

* 多协议支持

  * RMI
  * Hession
  * Dubbo
  * Thrift

  ```xml
  	<!-- 多协议支持RMI、Hessian、Dubbo、Thrift. host:指定服务发布本地ip地址 -->
  	<dubbo:protocol name="dubbo" port="20880"></dubbo:protocol>
  
  	<dubbo:protocol name="hessian" port="8087"></dubbo:protocol>
  ```

* 服务多版本支持

  ```xml
  	<!-- version 多版本支持 -->
  	<dubbo:service scope="" interface="com.denny.dubbo.IZkHello" ref="zkHelloService" protocol="dubbo,hessian" version="1.0.0"/>
  
  	<bean id="zkHelloService" class="com.denny.dubbo.impl.ZkHelloImpl" ></bean>
  
  
  	<dubbo:service interface="com.denny.dubbo.IZkHello" ref="zkHelloServiceV2" protocol="dubbo,hessian" version="1.0.1"/>
  
  	<bean id="zkHelloServiceV2" class="com.denny.dubbo.impl.ZkHelloV2Impl" ></bean
  ```

配置样例

```xml
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://dubbo.apache.org/schema/dubbo"
	xsi:schemaLocation="http://www.springframework.org/schema/beans
						http://www.springframework.org/schema/beans/spring-beans-4.3.xsd
						http://dubbo.apache.org/schema/dubbo
						http://dubbo.apache.org/schema/dubbo/dubbo.xsd">
	
	<!-- 提供方信息 -->
	<dubbo:application name="dubbo-server" owner="denny"/>

	<!-- 延迟服务发布
	<dubbo:provider delay="0"></dubbo:provider> -->
	
	<!-- 注册中心 无注册中心
	<dubbo:registry address="N/A"></dubbo:registry> -->

	<!-- 注册中心 zookeeper注册 file:指定协议地址缓存地址 -->
	<dubbo:registry address="zookeeper://192.168.3.14:2181" file="／Users/denny/data/dubboCache"></dubbo:registry>

	<!-- 多协议支持RMI、Hessian、Dubbo、Thrift. host:指定服务发布本地ip地址 -->
	<dubbo:protocol name="dubbo" port="20880"></dubbo:protocol>

	<dubbo:protocol name="hessian" port="8087"></dubbo:protocol>


	<!-- version 多版本支持 -->
	<dubbo:service scope="" interface="com.denny.dubbo.IZkHello" ref="zkHelloService" protocol="dubbo,hessian" version="1.0.0"/>

	<bean id="zkHelloService" class="com.denny.dubbo.impl.ZkHelloImpl" ></bean>


	<dubbo:service interface="com.denny.dubbo.IZkHello" ref="zkHelloServiceV2" protocol="dubbo,hessian" version="1.0.1"/>

	<bean id="zkHelloServiceV2" class="com.denny.dubbo.impl.ZkHelloV2Impl" ></bean>

</beans>
```



###Dubbo客户端

配置参数：

* 提供方信息

* 远程方法代理(包括：协议、容错、服务降级、服务版本配置等)

  容错机制-cluster：

  ```
  failsafe 失败安全，可以以为是把错误吞掉（记录日志）
  failover（默认）重试其他服务器。retries(重试次数默认为：2次)
  failfast 快速失败。失败后立马返回，适用于是事物性服务
  failback 失败后自动恢复.记录失败请求定时重发
  broadcast 广播，任意一台服务器报错，则执行的方法报错
  forking  设置并行数 forks
  ```

  服务降级（Mock）

  ```xml
      <!-- 服务降级 保证核心服务正常运行 mock -->
      <!-- 此时容错机制为：failsafe 错误被吞掉 直接返回null，即使设置了mock也不会执行 -->
      <dubbo:reference id="zkHelloService" interface="com.denny.dubbo.IZkHello" protocol="dubbo" version="1.0.1"
      cluster="failsafe"
                       timeout="1" mock="com.denny.dubbo.impl.ZkHelloMock"> </dubbo:reference>
  ```

  

  ```xml
      <!-- 此时容错机制为：failover 执行超时重试，最终执行mock -->
      <dubbo:reference id="zkHelloService" interface="com.denny.dubbo.IZkHello" protocol="dubbo" version="1.0.1"
                       cluster="failover"
                       timeout="1" mock="com.denny.dubbo.impl.ZkHelloMock"> </dubbo:reference>
  ```

配置样例：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://dubbo.apache.org/schema/dubbo"
       xsi:schemaLocation="http://www.springframework.org/schema/beans
						http://www.springframework.org/schema/beans/spring-beans-4.3.xsd
						http://dubbo.apache.org/schema/dubbo
						http://dubbo.apache.org/schema/dubbo/dubbo.xsd">

    <!-- 提供方信息 -->
    <dubbo:application name="dubbo-client" owner="denny"/>

    <!-- 注册中心 无注册中心
    <dubbo:registry address="N/A"></dubbo:registry>-->

    <!-- 注册中心 zookeeper注册-->
    <dubbo:registry address="zookeeper://192.168.3.14:2181"></dubbo:registry>

    <!-- 远程服务代理 无注册中心指定协议地址 配置url为无注册中心
    <dubbo:reference id="zkHelloService" interface="com.denny.dubbo.IZkHello" url="dubbo://192.168.3.14:20880/com.denny.dubbo.IZkHello"></dubbo:reference>
    -->

    <!-- 指定协议 protocol:duboo、hessian -->
    <!-- check="false" 启动配置检查 解决循环依赖 -->
    <!-- 容错机制 cluster：
         failsafe 失败安全，可以以为是把错误吞掉（记录日志）
         failover（默认）重试其他服务器。retries(重试次数)
         failfast 快速失败。失败后立马返回，适用于是事物性服务
         failback 失败后自动恢复.记录失败请求定时重发
         broadcast 广播，任意一台服务器报错，则执行的方法报错
         forking  设置并行数 forks
    -->
    <!-- 服务多版本支持 version-->

    <!-- 服务降级 保证核心服务正常运行 mock -->
    <!-- 此时容错机制为：failsafe 错误被吞掉 直接返回null，即使设置了mock也不会执行
    <dubbo:reference id="zkHelloService" interface="com.denny.dubbo.IZkHello" protocol="dubbo" version="1.0.1"
    cluster="failsafe"
                     timeout="1" mock="com.denny.dubbo.impl.ZkHelloMock"> </dubbo:reference> -->

    <!-- 此时容错机制为：failover 执行超时重试，最终执行mock -->
    <dubbo:reference id="zkHelloService" interface="com.denny.dubbo.IZkHello" protocol="dubbo" version="1.0.1"
                     cluster="failover"
                     timeout="1" mock="com.denny.dubbo.impl.ZkHelloMock"> </dubbo:reference>
</beans>
```

### 配置优先级

以timeout为例，显示了配置的查找顺序，其它retries, loadbalance等类似。

* 方法级优先，接口级次之，全局配置再次之。

* 如果级别一样，则消费方优先，提供方次之。

其中，服务提供方配置，通过URL经由注册中心传递给消费方。

建议由服务提供方设置超时，因为一个方法需要执行多长时间，服务提供方更清楚，如果一个消费方同时引用多个服务，就不需要关心每个服务的超时设置。

### Dubbo原理

#### Java SPI

SPI全称（service provider interface），是JDK内置的一种服务提供发现机制，目前市面上有很多框架都是用它来做服务的扩展发现。动态替换发现机制。一种插拔式的扩展手段。

#### SPI 规范

实现SPI，就需要按照SPI本身定义的规范来进行配置，SPI规范如下

*  需要在classpath下创建一个目录，该目录命名必须是：META-INF/services

* 在该目录下创建一个properties文件，该文件需要满足以下几个条件

* * 文件名必须是扩展的接口的全路径名称

* * 文件内部描述的是该扩展接口的所有实现类

* * 文件的编码格式是UTF-8

* 通过java.util.ServiceLoader的加载机制来发现

#### Dubbo SPI

规范如下：

* 需要在resource目录下配置META-INF/dubbo或者META-INF/dubbo/internal或者META-INF/services，并基于SPI接口去创建一个文件。

* 文件名称和接口名称保持一致，文件内容和SPI有差异，内容是KEY对应Value。

#### 消费端负载均衡

LoadBalance算法：

随机加权重（`RandomLoadBalance`）

一致性Hash负载均衡(`ConsistentHashLoadBalance`)

最小活跃数负载均衡(`LeastActiveLoadBalance`)

轮询负载均衡(`RoundRobinLoadBalance`)

## Kafka

分布式消息和订阅系统。高性能、高吞吐量（每秒请求处理几十万条数据）。Scala语言编写，非JMS规范。

### 架构

![image-20190224143330667](/Users/denny/Library/Application Support/typora-user-images/image-20190224143330667.png)

多个broker（消息提供者），采用zookeeper集群实现broker的master-slave集群。Consumer从broker中pull（拉取）消息。

Topic：主题。

Partition：topic中的分区。

Group：消费端的所属。

* 生产者参数：

* * ProducerConfig.ACKS_CONFIG = "-1"

    0：消息发送给broker后，不需要确认（性能高，但是会出现数据丢失）。

    1：只需要获得kafka集群中leader节点的确认即可返回。（leader/follower）

    all（-1）：需要ISR中的所有副本进行确认（需要集群中的所有节点确认）。性能低但数据最安全， 也可 能会出现数据丢失（但ISR中的副本只有一个）。

* * ProducerConfig.BATCH_SIZE_CONFIG(batch.size) 

    Producer对于同一个分区来说，会按照batch.size的大小进行统一收集进行批量发送。默认16KB。

* * ProducerConfig.LINGER_MS_CONFIG(linger.ms) 

    每次发送间隔。与(batch.size)配合使用在指定毫秒时间间隔中延迟(delay)，收集发送的消息，进行批量发送到broker。默认为0。

    如batch.size 和 linger.ms都配置只要满足其一就会批量发送。

* *  ProducerConfig. MAX_REQUEST_SIZE_CONFIG (max.request.size)  
    发送请求最大长度。默认为：1MB。设置合理提高发送和接送性能。
    消息同步和异步发送：
    Kafka-1.0版本以后默认为异步发送消息。

    Kafka持久化消息，即磁盘中。

* 接收者参数：

  * ConsumerConfig.GROUP_ID_CONFIG(group.id)

  ​	不同的groupId分组的消费者都可以接受到消息，但同一组中的消费者只有其中一个可以接受到消息。

  * ConsumerConfig. AUTO_OFFSET_RESET_CONFIG

    对于新的groupId来说，如果设置为：earliest，那么它会从最早的消息开始消费。对于已消费的groupId来说则从已消费的最大offset开始消费消息。
                      earliest：对于新的groupId重置offset。
                      none：如果新的groupId无offset(即没有消费过消息)抛出异常。
                      latest：对于新的groupId取最近的消息（即：已消费并且提交的最大的offset）。

  * ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG
    自动提交(ACK)。值为：true/false。
                    ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG
                    (auto.commit.interval.ms)
                    自动提交间隔毫秒数。批量确认。

  * ConsumerConfig.MAX_POLL_RECORDS_CONFIG(max.poll.records) 

    每次poll拉取的消息最大数量。

### Topic 与分区

Topic 是存储消息的逻辑概念。

Partition

* 每个topic可以划分多个分区。至少有一个分区。

* 相同topic下的不同分区中的消息是不同的。

Partition的命名方式：Topic名称-递增序号(从0开始)。如：test-0、test-1、test-2。

### 消息分发策略

存储方式：KEY-VALUE

key：可选项

根据key按照分区策略将消息放入指定分区中。

自定义分区策略：

实现org.apache.kafka.clients.producer.Partitioner接口
默认分区算法为：hash取模算法。

### 消息消费原理

多个消费者如何分配分区(partition)。

消费者可以指定消费某一个分区的消息。

同一groupId的消费者会均匀消费多个分区中的消息。如：topic：test中有三个分区为：P0、P1及P2，有三个消费者C1、C2、C3在同一groupId中消费topic=test中消息。那么消费者将均匀消费P0、P1、P2分区中的消息。如：C0消费分区P1;C1消费分区P0;C2消费分区P2。

分几种情况：

Partition数量 = 消费者数量：均匀消费

Partition数量 < 消费者数量：存在消费者不消费消息

Partition数量 > 消费者数量：存在消费者消费多个分区

同一Partition中不存在并发。

### Rebalance

什么时候触发Rebalance(分区分配策略)？

*  对于同一Consumer Group中新增消费者。

* 消费者离开Consumer Group

* Topic中新增了Partition

* 消费者主动取消订阅Topic

重新规划Consumer消费Partition。

分区分配策略：

* Range(范围)-默认方式
* RoundBobin(轮询)

### 消息的存储策略

消息保存路径：topic->partition0

​				    ->partition1

​                                    ->partition2

存放到kafka-logs目录：如test-0、test-1、test-3即：Topic test的分区0、分区1、分区2。

消息写入性能：

按照顺序写入。零拷贝（IO）。

消息存储策略:

LogSegment：分段保存。文件达到预设值重新生成。

清理策略：

根据时间：消息的保存时间，默认为：7天。配置参数：log.retention.hours=168

根据日志文件大小。配置参数：log.retention.bytes=1073741824

压缩策略：

根据相同的KEY取最新的值。

### Partition副本策略

同一Partiton存在多个副本。

## Redis

分布式缓存

缓存作用：低速IO和高速应用的差异。

承载数据吞吐量，降低数据存储成本，高速访问数据。

### 数据类型

![image-20190224175608407](/Users/denny/Library/Application Support/typora-user-images/image-20190224175608407.png)

字符串类型

应用场景：IP限制(INCR)、Session存储、短信验证。

List类型

应用场景：分布式队列(lpush和rpop)、栈(后进先出lpush和lpop)、消息队列(lpush和brpop)。

Hash Map类型

应用场景：储存二维数据

Set类型

 不允许重复、无序。

应用场景：去重、用户标签（千人千面推荐）、使用并集、差集进行统计如共同好友等。

可排序Set类型

应用场景：PV排序热门文章、根据时间排序新闻列表等。

### 过期时间的设置和原理

* expire key seconds

  * ttl key 监控过期时间
  * persist key 取消过期时间

* setex(Stringkey, int seconds,String value)

* 原理：

  * 消极方法（passive way）

    当应用访问key时发现已过期则删除。

  *  积极方法（active way）

    周期性从设置了过期时间的key中选择一部分的key进行删除。

  * * 随机测试20个带有timeout信息的key
    * 删除发送过期的key
    *  如果超过25%的key过期删除，则重复整个流程

### 发布订阅模式

![image-20190224175457958](/Users/denny/Library/Application Support/typora-user-images/image-20190224175457958.png)

​	pub/sub模式

​	生产者发布消息：

​	publish channel message

![image-20190224175715480](/Users/denny/Library/Application Support/typora-user-images/image-20190224175715480.png)

​	订阅主题：

​	subscribe channe

​	   ![image-20190224175754987](/Users/denny/Library/Application Support/typora-user-images/image-20190224175754987.png)

​	只能作为简单的发布-订阅模式，不支持多协议、不支持消息持久化存在消息丢失、不能保障可靠性、不支持

​	事务等。可以用于站内简单通信如聊天工具等。

###Redis持久化及原理分析

RDB、AOF

RDB：

当符合【条件】的时候，fork子进程，生成rdb.dump.rdb快照文件。

*  配置规则，redis.conf配置参数：

  ```
  save seconds changes
  
  save 900 1          //900秒内有一个或以上的key被更改过
  
  save 300 10         
  
  save 60 10000
  ```

  满足以上其中一个条件就会触发生成快照文件。

*  用户主动执行save或者bgsave。

​	save：会阻塞所有客户端的请求。

​	bgsave：后台异步快照任务

*  flushall 清空内存中所有数据。执行时如快照规则配置存在就会执行快照任务。

* 执行复制操作。

  两次快照间隔期间，执行了内存操作会存在数据丢失。

AOF：

实时日志，每执行事务性操作会把执行变更的数据写入到磁盘中，消耗性能，保证数据尽

可能不丢失。在redis.conf配置参数：

```properties
appendonly yes
appendfilename “appendonly.aof”
```

如果RDB和AOF同时开启以启用AOF的快照文件加载数据。AOF快照文件会记录所有

的数据事务操作日志，AOF文件会很大但实际数据量并不大。如何优化？

```properties
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb 
```

auto-aof-rewrite-percentage：当前AOF文件大小超过上次重写文件的指定百分比时触发

重写。 默认100%。

auto-aof-rewrite-min-size：AOF文件允许重写的最小长度。

AOF重写(fork子进程进行)：重写可以去除数据的中间执行过程，直接保留最终数据命令。

参数appendfsync，在Redis中对AOF调用write写入后，何时再调用fsync将其写到

磁盘上，通过appendfsync选项来控制

```properties
appendfsync everysec //默认每隔一秒进行一次fsync调用，将缓冲区中的数据写到磁盘
appendfsync always //每一次写操作都会调用一次fsync，这时数据是最安全的，性能受影响
appendfsync no //不主动调用fsync去将AOF日志内容同步到磁盘，依赖操作系统

```

###Redis的内存回收策略

LRU最近最少使用,淘汰数据。

LFU根据数据的历史访问频率来淘汰数据，如果数据过去被访问多次，那么将来被访问的

频率也更高。访问频率最低的淘汰。

```properties
maxmemory-policy 
noeviction：不淘汰删除数据，如果内存不足，写入时返回错误。
volatile-lru：淘汰设置了过期时间并且最近最少使用的数据
allkeys-lru：最近最少使用数据淘汰
volatile-lfu：
allkeys-lfu：
volatile-random：随机淘汰设置了过期时间的key
allkeys-random：随机key淘汰
volatile-ttl：淘汰即将过期的数据。

```

### Redis单线程为什么性能很高

* 内存和网络的宽带，多路复用。同时处理大量的客户端请求。单线程避免多线程竞争和上下文切换。

###Lua脚本在Redis中的应用

pileline

使用Lua脚本保证多个客户端执行多个命令的复用性、原子性、有序性和减少网络开销。

Lua安装

```shell
curl -R -O http://www.lua.org/ftp/lua-5.3.5.tar.gz 
tar zxf lua-5.3.5.tar.gz 
cd lua-5.3.5 
make linux test或make macosx test
sudo make install

```

在Lua脚本中去执行Redis命令：

```shell
redis.call()
```

eval 执行脚本：

```shell
./redis-cli   --eval /Users/denny/Documents/LuaScripts/ratelimit.lua 192.168.3.14 , 10 10  
```

ratelimit.lua 脚本：

```shell
local key = "ratelimit:"..KEYS[1]

local limit = tonumber(ARGV[1])

local expireTime = ARGV[2]

local times = redis.call('incr', key)

if times == 1 then
   redis.call('expire', key, expireTime)
end

if times > limit then
   return 0
end

return 1

```

客户端执行编写脚本执行：

![image-20190224170052422](/Users/denny/Library/Application Support/typora-user-images/image-20190224170052422.png)

![image-20190224170058448](/Users/denny/Library/Application Support/typora-user-images/image-20190224170058448.png)

### Redis集群

解决单点问题。读写分离。

![image-20190224175430388](/Users/denny/Library/Application Support/typora-user-images/image-20190224175430388.png)

主从复制方案。

```
192.168.3.37-Master

192.168.3.38-Slave

192.168.3.39-Slave
```

在Slave节点的redis.conf中配置maser信息：

```shell
 slaveof 192.168.3.37 6379  
```

在Master和Slave节点的redis.conf中配置取消本机绑定和保护模式：

```shell
   protected-mode no   #bind 127.0.0.1   
```

集群成功启动：

```shell
>info replication #查看集群情况
```

![image-20190224170404364](/Users/denny/Library/Application Support/typora-user-images/image-20190224170404364.png)

Slave客户端不能执行写入操作：

![image-20190224170501542](/Users/denny/Library/Application Support/typora-user-images/image-20190224170501542.png)

复制方式：

* 全量复制

  * 初始化时，如新增slave节点。

    RDB快照

* 增量复制

  * Slave监听增量复制：

    ```shell
    > replconf listener-port 6379 
    
    >sync
    ```

    如下：

    ![image-20190224170659215](/Users/denny/Library/Application Support/typora-user-images/image-20190224170659215.png)

    Master上的配置：

    ```shell
    min-slaves-to-write 3   //当master连接上3个或以上slave才可写
    min-slaves-max-lag 10  //slave最长丢失连接的时间(秒)
    ```

    repl_backlog和slave_repl_offset保证slave与master断开重连上后从上次同步的offset开始增量同步。

* 无磁盘复制

  repl-diskless-sync no //指定不生成RBD快照进行全量同步，在内存中生成RBD文件。

###哨兵机制

实现集群高可用。

Master选举，哨兵机制。Master 宕机后从Slave中选举Master。

![image-20190224175304878](/Users/denny/Library/Application Support/typora-user-images/image-20190224175304878.png)

作用：

* 监控master和slave是否正常运行。
* 当master出现故障时，从slave中选举新的master。

 Raft 实现分布式一致性的协议。

 在Master 和 Slave节点上配置哨兵进程，192.168.3.37作为master。

 哨兵机制相关配置（sentinel.conf）：

```shell
bind 0.0.0.0
protected-mode no
port 26397
dir "/tmp"
sentinel monitor mymaster 192.168.3.37 6379 2 
sentinel auth-pass mymaster 012_345^678-90
sentinel down-after-milliseconds mymaster 60000 
sentinel can-failover mymaster yes 
sentinel failover-timeout mymaster 180000 
sentinel parallel-syncs mymaster 1  

```

   依次启动master和slave节点的redis服务：

```
$ ./redis-server ../redis.conf
```

   在master和slave节点上依次启动哨兵进程：

```
$ ./redis-sentinel ../sentinel.conf
```

如图：

![image-20190224172104305](/Users/denny/Library/Application Support/typora-user-images/image-20190224172104305.png)

![image-20190224172112494](/Users/denny/Library/Application Support/typora-user-images/image-20190224172112494.png)

![image-20190224172122949](/Users/denny/Library/Application Support/typora-user-images/image-20190224172122949.png)

故障转移

![image-20190224172137632](/Users/denny/Library/Application Support/typora-user-images/image-20190224172137632.png)

![image-20190224172150253](/Users/denny/Library/Application Support/typora-user-images/image-20190224172150253.png)

### Redis Cluster

![image-20190224175445066](/Users/denny/Library/Application Support/typora-user-images/image-20190224175445066.png)

数据分片、路由。一致性Hash->codis->redis cluster。

在redis-cluster架构中，redis-master节点一般用于接收读写，而redis-slave节点则一般只用于备份。但如    

果不介意读取的是redis-cluster中有可能过期的数据并且对写请求不感兴趣时，则可以通过readonly命令

将slave节点设置为只读模式接受读请求，以实现读写分离。

192.1683.37、192.168.3.38、192.168.3.39搭建Redis Cluster。采用三组一主(Master)二从(Slave)。

端口配置：7000、7001、7002、7003、7004、7005、7006、7007、7008、7001、7001、7001、7002

*  在192.1683.37、192.1683.38、192.1683.39的 $HOME（即：/home/denny）目录下建立如下目录：192.1683.37：/$HOME/redis-cluster/7000/、/$HOME/redis-cluster/7001/、/$HOME/redis-cluster/7002/

  192.1683.38：/$HOME/redis-cluster/7003/、/$HOME/redis-cluster/7004/、/$HOME/redis-cluster/7005/

  192.1683.39：/$HOME/redis-cluster/7006/、/$HOME/redis-cluster/7007/、/$HOME/redis-cluster/7008/

*  在192.1683.37、192.1683.38、192.1683.39的redis安装目录 （即：/home/denny/Software/redis-4.0.12）下建立如下目录：

  192.1683.37： /redis-cluster/7000/、/redis-cluster/7001/、/redis-cluster/7002/

  192.1683.38： /redis-cluster/7003/、/redis-cluster/7004/、/redis-cluster/7005/

  192.1683.39： /redis-cluster/7006/、/redis-cluster/7007/、/redis-cluster/7008/

  在192.168.3.37服务器上拷贝一份/home/denny/Software/redis-4.0.12/目录下的redis.conf配置文件d
  到：/home/denny/Software/redis-4.0.12/redis-cluster/7000/目录下。修改redis .conf文件中的配置参数如下：

  ```properties
  daemonize yes #开启后台运行 
  port 7000 #工作端口 
  bind 192.168.3.37 #绑定机器的内网IP,一定要设置呀老铁，不要用127.0.0.1 
  dir /home/denny/redis-cluster/7000/ #指定工作目录，rdb,aof持久化文件将会放在该目录下，不同实例一定要配置不同的工作目录 
  cluster-enabled yes #启用集群模式 
  cluster-config-file nodes-7000.conf #生成的集群配置文件名称，集群搭建成功后会自动生成，在工作目录下 
  cluster-node-timeout 5000 #节点宕机发现时间，可以理解为主节点宕机后从节点升级为主节点时间 appendonly yes #开启AOF模式 
  pidfile /var/run/redis_7000.pid #pid file所在目录
  
  ```

  同时将/home/denny/Software/redis-4.0.12/redis-cluster/7000/redis.conf文件拷贝到对应的/7001/和/7002/目录下。执行如下命令修改文件中的端口号：

  ```
  $ sed -i 's/7000/7001/g' ./7001/redis.conf
  $ sed -i 's/7000/7002/g' ./7002/redis.conf
  ```

  将/7000/目录下的redis.conf文件使用scp命令拷贝到192.168.3.38和192.168.3.39服务器上/home/denny/Software/redis-4.0.12/redis-cluster/目录下各个端口命名目录下。参考以上方式在192.168.3.38和192.168.3.39服务器上修改各个不同端口对应的Redis实例的redis.conf，命令如下：

  192.168.3.38

  ```
  $ sed -i 's/7000/7003/g' ./7003/redis.conf
  $ sed -i 's/7000/7004/g' ./7004/redis.conf
  $ sed -i 's/7000/7005/g' ./7005/redis.conf
  
  $ sed -i 's/3.37/3.38/g' ./7003/redis.conf
  $ sed -i 's/3.37/3.38/g ' ./7004/redis.conf
  $ sed -i 's/3.37/3.38/g ' ./7005/redis.conf
  
  ```

  192.168.3.39

  ```
  $ sed -i 's/7000/7006/g' ./7006/redis.conf
  $ sed -i 's/7000/7007/g' ./7007/redis.conf
  $ sed -i 's/7000/7008/g' ./7008/redis.conf
  
  $ sed -i 's/3.37/3.39/g' ./7006/redis.conf
  $ sed -i 's/3.37/3.39/g ' ./7007/redis.conf
  $ sed -i 's/3.37/3.39/g ' ./7008/redis.conf
  
  ```

* 在192.1683.37、192.1683.38、192.1683.39服务器上安装Ruby和RubyGems 。在GentOS-7使用yum安装的Ruby版本是2.0.0版本，但是redis中构建集群的redis-trib.rb工具需要>= 2.2.2版本。

  ![image-20190224172639503](/Users/denny/Library/Application Support/typora-user-images/image-20190224172639503.png)

  使用如下方式安装ruby-2.2/2.3/2.4以上版本：

   安装ruby-2.3版本

  ```shell
  # yum install centos-release-scl-rh
  # yum install rh-ruby23  -y
  $scl enable  rh-ruby23 bash
  ```

  ```shell
  # yum install ruby
  # yum install rubygems
  # gem install redis --version 3.3.3
  ```

* 在192.1683.37、192.1683.38、192.1683.39服务器上安装redisgem，如下：

  ```
  $ gem install redis
  ```

* 在192.1683.37、192.1683.38、192.1683.39服务器上启动各个redis实例，如下：

  192.168.3.37的/home/denny/Software/redis-4.0.12/redis-cluster目录下执行

  ```shell
  $ ../src/redis-server ./7000/redis.conf
  $ ../src/redis-server ./7001/redis.conf
  $ ../src/redis-server ./7002/redis.conf
  ```

  192.168.3.38的/home/denny/Software/redis-4.0.12/redis-cluster目录下执行

  ```
  $ ../src/redis-server ./7003/redis.conf
  $ ../src/redis-server ./7004/redis.conf
  $ ../src/redis-server ./7005/redis.conf
  ```

  192.168.3.39的/home/denny/Software/redis-4.0.12/redis-cluster目录下执行

  ```
  $ ../src/redis-server ./7006/redis.conf
  $ ../src/redis-server ./7007/redis.conf
  $ ../src/redis-server ./7008/redis.conf
  ```

  查看redis启动，如图：

  ![image-20190224172929922](/Users/denny/Library/Application Support/typora-user-images/image-20190224172929922.png)

* 在192.1683.37、192.1683.38、192.1683.39服务器中的一台上使用redis-trib.rb工具创建集群，命令如下：

  192.168.3.39的/home/denny/Software/redis-4.0.12/redis-cluster目录下执行

  ```shell
  $ ./redis-trib.rb create --replicas 2 192.168.3.37:7000 192.168.3.37:7001 192.168.3.37:7002 192.168.3.38:7003 192.168.3.38:7004 192.168.3.38:7005 192.168.3.39:7006 192.168.3.39:7007 192.168.3.39:7008
  ```

  如图：

  ![image-20190224173031159](/Users/denny/Library/Application Support/typora-user-images/image-20190224173031159.png)

  ​	修改进程mit -n 65535 >>/etc/profile 

  ​	 集群示意图：

  ![image-20190224173105707](/Users/denny/Library/Application Support/typora-user-images/image-20190224173105707.png)

* 测试集群

  重定向

  ![image-20190224173554645](/Users/denny/Library/Application Support/typora-user-images/image-20190224173554645.png)

  集群节点信息：

  ![image-20190224173610440](/Users/denny/Library/Application Support/typora-user-images/image-20190224173610440.png)

  从节点数据同步：

  ![image-20190224173626176](/Users/denny/Library/Application Support/typora-user-images/image-20190224173626176.png)

  故障转移测试：

  先查看Master信息：

  ![image-20190224173646990](/Users/denny/Library/Application Support/typora-user-images/image-20190224173646990.png)

  使用DEBUG SEGFAULT 命令搞挂7007 节点：

  ![image-20190224173706033](/Users/denny/Library/Application Support/typora-user-images/image-20190224173706033.png)

  再次查看master信息：

  ![image-20190224173718679](/Users/denny/Library/Application Support/typora-user-images/image-20190224173718679.png)

  7007Master节点转移到了 7004节点。

*   常用监控命令

  info server

  info replication

  info memory

  info persistence

  info stats

  info cup

  info cluster

   

  cluster nodes

  cluster info

  cluster meet

  cluster forget

### Redis的应用

* Redis Java客户端：jedis、redisson、lettuce。

  Jedis 通过socket建立会话，提供了线程池。

  Redisson实现了基本命令，同时提供了分布式锁、队列、原子递增等应用功能的封装。

  Lettuce基于Netty构建可伸缩的客户端。支持同步、异步和响应式。

* 分布式锁：

  redis setnx

  SETNX：当且仅当 key 不存在，设置给定的value。若给定的 key 已经存在，则 [SETNX](http://redisdoc.com/string/setnx.html#setnx) 不做任何

  动作。

  锁的基本操作：获取锁、释放锁、超时时间（避免死锁）、判断是否重入（释放锁的线程是否为已获取锁的线程）。

*  Redis事务操作：

  Multi、Exec、Discard和Watch命令是Redis 事务功能的基础。Redis事务允许在一次单独的步骤中执行一   

  组命令，保证两个重要事项：

  * Redis会将一个事务中的所有命令序列化，然后按顺序执行。Redis不可能在一个Redis事务的执行过

  程中插入执行另一个客户端发出的请求。这样便能保证Redis将这些命令作为一个单独的隔离操作执行。

  * 在一个Redis事务中，Redis要么执行其中的所有命令，要么什么都不执行。因此，Redis事务能够保证原子性。EXEC命令会触发执行事务中的所有命令。

    Multi：用于标记事务开始。

    Exec：在一个事务中执行所有先前放入队列的命令，然后恢复正常的连接状态。

    Discard：清除所有先前在一个事务中放入队列的命令，然后恢复正常的连接状态。

    Watch：当某个事务需要按条件执行时，就要使用这个命令将给定的键设置为可控的。

    Unwatch：清除所有先前为一个事务监控的键。

    使用方法：

    使用MULTI命令便可以进入一个Redis事务。这个命令的返回值总是OK。此时，用户可以发出多个Redis命令。Redis会将这些命令放入队列，而不是执行这些命令。一旦调用EXEC命令，那么Redis就会执行事务中的所有命令。

    相反，调用DISCARD命令将会清除事务队列，然后退出事务。

  

  ​	Redis使用WATCH命令实现事务的“检查再设置”（CAS）行为，实现乐观锁。

  ​	作为WATCH命令的参数的键会受到Redis的监控，Redis能够检测到它们的变化。在执行EXEC命令之前，

  ​	如果Redis检测到至少有一个键被修改了，那么整个事务便会中止运行，然后EXEC命令会返回一个Null

  ​	值，提醒用户事务运行失败。

  ​	WATCH命令详解

  ​	那么WATCH命令实际做了些什么呢？这个命令会使得EXEC命令在满足某些条件时才会运行事务：我们

  ​	要求Redis只有在所有受监控的键都没有被修改时，才会执行事务。（但是，相同的客户端可能会在事务	

  ​	内部修改这些键，此时这个事务不会中止运行。）否则，Redis根本就不会进入事务。（注意，如果你使	

  ​	用WATCH

  ​	命令监控一个易失性的键，然后在你监控这个键之后，Redis再使这个键过期，那么EXEC命令仍然可以

  ​	正常工作。）

  ​	WATCH命令可以被调用多次。简单说来，所有的WATCH命令都会在被调用之时立刻对相应的键进行监

  ​	控，直到EXEC命令被调用之时为止。你可以在单条的WATCH命令之中，使用任意数量的键作为命令参

  ​	数。

  ​	当调用EXEC命令时，所有的键都会变为未受监控的状态，Redis不会管事务是否被中止。当一个客户单

  ​	连接被关闭时，所有的键也都会变为未受监控的状态。

   

  ​	你还可以使用UNWATCH命令（不需要任何参数），这样便能清除所有的受监控键。当我们对某些键施加

  ​	乐观锁之后，这个命令有时会非常有用。因为，我们可能需要运行一个用来修改这些键的事务，但是在读

  ​	取这些键的当前内容之后，我们可能不打算继续进行操作，此时便可以使用UNWATCH命令，清除所有受

  ​	监控的键。在运行UNWATCH命令之后，Redis连接便可以再次自由地用于运行新事务。

* Pipeline 管道模式

  客户端一次性提交多个操作命令，无需等待服务端返回。大大减少了网络往返时间，提供系统性能。

### Redis应用中的问题

![image-20190224175326523](/Users/denny/Library/Application Support/typora-user-images/image-20190224175326523.png)

* 数据一致性
* 最终一致性

* 缓存雪崩

  如果缓存数据很大，同时大量的KEY存在相同的过期时间。会导致在同一时刻大量的缓存数据过期失效。此时会有大量的请求穿透到数据库层，导致数据库压力过大而故障。

  * 从缓存中取不到值时加锁排队， 通过加锁或排队来控制读数据库写缓存的线程数量。比如对某一key只允许一个线程查询数据和写缓存，其他线程等待。SETNX实现锁。

  * 缓存过期时间均匀分布

  * 使用多级缓存，如二级缓存，或双缓存策略。

    内存缓存、Redis。

* 缓存击穿

  缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。

  * 对空值进行缓存。缓存空对象，将null变成一个值。

    也可以采用一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。

    * 缓存空对象会有两个问题：

    * * 第一，空值做了缓存，意味着缓存层中存了更多的键，需要更多的内存空间 ( 如果是攻击，问题更严重 )，比较有效的方法是针对这类数据设置一个较短的过期时间，让其自动剔除。

      * 第二，缓存层和存储层的数据会有一段时间窗口的不一致，可能会对业务有一定影响。例如过期时间设置为 5分钟，如果此时存储层添加了这个数据，那此段时间就会出现缓存层和存储层数据的不一致，此时可以利用消息系统或者其他方式清除掉缓存层中的空对象。

  *  设置key 的规则

    不满足规则不请求后端。

  * 布隆过滤器

    空间效率非常高的概率性算法。

    存储40亿数据，一个数据占4个比特位，内存需要16G。

    Bitmap （位图）

    int 4个字节-32个比特位（可以存储32个十进制的数据）

## RabbitMQ

### RabbitMQ的架构图

![image-20190224180134010](/Users/denny/Library/Application Support/typora-user-images/image-20190224180134010.png)

* 直连交换机

  ![image-20190224180234234](/Users/denny/Library/Application Support/typora-user-images/image-20190224180234234.png)

* 主题交换机

  ![image-20190224180250843](/Users/denny/Library/Application Support/typora-user-images/image-20190224180250843.png)

* 广播交换机

  ![image-20190224180300074](/Users/denny/Library/Application Support/typora-user-images/image-20190224180300074.png)

### MQ应用场景

异步、解耦、削峰

* 跨系统的异步通信

* 应用内的同步变成异步

  秒杀

* 给予Pub/Sub模型实现的事件驱动

  ETL

* 利用RabbitMQ实现事物的最终一致性

### AMQP协议

​	AMQP，即：Advanced Message Queuing Protocol，一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件产品、不同的开发语言等条件的限制。

AMQP的实现有：RabbitMQ、Apache Qpid等。

### 进阶知识

* 怎么自动删除没人消费的消息

  避免大量消息堆积。

  * 设置队列的消息的TTL（Time To Live 存活时间）
  * 指定消息的TTL

* 无法路由的消息，去了哪里？

  死信队列（Dead Letter Queue）。对应的交换机：DLX（Dead Letter Exchange）。

  那些情况消息会变成死信？

  * 消息过期
  * 消费者Reject或Nack并且requeue== false（即：拒绝或未应答，并且拒绝重新入队）
  * 对列达到最大长度 (数量或字节数)：先入队的消息会被删除。
  * 无法路由的消息进入死信队列。

* 可以让消息优先得到消费吗？

  优先级队列：

  在发送端设置消息的优先级如：

  ```java
  AMQP.BasicProperties properties = new AMQP.BasicProperties.Builder()
          . priority (5) //优先级，默认为：5，配合队列的x-max-priority 属性使用
          .build()；
  ```

  设置队列的x-max-priority属性如：

  ```java
  Map<String, Object> basicProperties = new LinkedHashMap<String, Object>();
  basicProperties.put("x-max-priority", "10");//最大优先级
  
  ```

* 如何实现延迟发送消息？

  RabbitMQ本身不支持延迟队列。

  如何实现？

  * 延迟队列的插件

  * 配合TTL和死信队列实现

* MQ如何实现RPC？

  采用请求队列和响应队列实现RPC远程方法调用。

* RabbitMQ流量控制怎么做？设置队列大小有用吗？

  秒杀系统。

  x-max-length：

  *  消息堆积的时候才有用。当消费速率与生产速率。

  *  先进入队列的消息删除掉。

  流量控制：

  * 服务端

    Flow Control 

    内存和磁盘大小剩余。

  *  消费端

    多个消费者监听一个队列。消息轮询消费。

    平均 != 公平

    设置prefetch count（预取数量）

    说明：非自动确认消息的前提下，如果一定数目的消息（通过基于consumer设置Qos的值）未被确认前，不进行消费新的消息。

    `channel.basicQos(3);`

### RabbitMQ可靠性投递

#### 可靠性投递​

* 确保消息发送到Broker
  * 服务端确认-Transaction模式
  * * channel.txSelect

* * * channel.txCommit

* * * channel.txRollback

* * 服务端确认-Confirm模式

* * * channel.confirmSelect

    * if(channel.waitForConfirms()){

      ​     //消息发送成功

    }

* 确保消息从队列正确投递到消费者
  * 消费者确认
  * * 自动确认
    * 手工确认
      * channel.basicAck //应答
      * channel.basicReject //单条拒绝
      * channel.basicNack //批量拒绝
    * 消息何时删除
      * 是消息发送给消费者之后
      * 消费者业务处理完

### 其他

*  消费者回调

  消息入库

  保证服务端可以知晓客户端已接受到消息并处理

* * 提供一个回调API

* * 消费者发送消息

* * 补偿机制

  消息生产者发送消息未收到客户端的响应信息，重发消息（如定时重发）。

  ATM存款、ATM取款

* 消息幂等性

  重发消息时保证消息幂等性。消费者，保证消息的唯一性，如采用唯一标识。接受到消息后先根据唯一标识校验消息是否已处理或已存在。

* 消息顺序性

  一个队列只有一个消费者时才能保证消息的顺序性。

## JVM

### JVM体系结构

![image-20190224203806604](/Users/denny/Library/Application Support/typora-user-images/image-20190224203806604.png)

### 类加载器

负责加载class文件，class文件在文件开头有特定的文件标示， 并且ClassLoader只负责class文件的加载，至于它是否可以运行，则由 Execution Engine决定 。

![image-20190224204017429](/Users/denny/Library/Application Support/typora-user-images/image-20190224204017429.png)

![image-20190224204121885](/Users/denny/Library/Application Support/typora-user-images/image-20190224204121885.png)

### Execution Engine

执行引擎负责解释命令，提交操作系统执行

### Native Interface本地接口

本地接口的作用是融合不同的编程语言为 Java 所用。

### Native Method Stack

它的具体做法是Native Method Stack中登记native方法，在Execution Engine 执行时加载本地方法库。

### PC寄存器

每个线程都有一个程序计数器，是线程私有的,就是一个指针， 

指向方法区中的方法字节码(用来存储指向下一条指令的地址,也即将 要执行的指令代码)，由执行引擎读取下一条指令，是一个非常小的内 存空间，几乎可以忽略不记。 

### 方法区

Method Area

方法区是被所有线程共享，所有字段和方法字节码，以及一 些特殊方法如构造函数，接口代码也在此定义。简单说，所有定 义的方法的信息都保存在该区域，此区属于共享区间。 

静态变量、常量 、类信息 、构造方法 、接口定义 、运行时常量池存在方法区中。

### 栈区

栈也叫栈内存，主管Java程序的运行，程序的运行，是在线程创建时创建，它的生命期是跟随线程的生命期，线程结束栈内存也就释放， 对于栈来说不存在垃圾回收问题，只要线程一结束该栈就 Over，生命周期和线程一致，是线程私有的。 8种基本类型的变量、对象的引用变量、实例方法都是在函数的栈内存中分配。

栈存储什么？

本地变量（Local Variables）：输入参数和输出参数以及方法内的变量。

栈操作（Operand Stack）：记录出栈、入栈的操作。

![image-20190224210502660](/Users/denny/Library/Application Support/typora-user-images/image-20190224210502660.png)

![image-20190224210622877](/Users/denny/Library/Application Support/typora-user-images/image-20190224210622877.png)

### 栈、方法区和堆的关系

![image-20190224210820592](/Users/denny/Library/Application Support/typora-user-images/image-20190224210820592.png)

### Heap 堆

一个JVM实例只存在一个堆内存，堆内存的大小是可以调节的。类加载器读取了类文件后，需要把类、方法、
常变量放到堆内存中，保存所有引用类型的真实信息，以方便执行器执行，堆内存分为三部分:

* Young Generation Space 新生区	Young/New
* Tenure generation space 养老区        Old/ Tenure
* Permanent Space              永久区       Perm

   Heap堆Java8

   一个JVM实例只存在一个堆内存，堆内存的大小是可以调节的。 类加载器读取了类文件后，需要把类、方法、常变量放到堆内存中，保存所有引用类型的真实信息，以方便执行器执行。

​	堆内存逻辑上分为三部分:新生、养老、方法区。

![image-20190224212239873](/Users/denny/Library/Application Support/typora-user-images/image-20190224212239873.png)

​     方法区( Method Area)，是各个线程共享的内存区域，它用于存储虚拟机加载的:类信息、普通常量、静态常量、编译器编译后的代码等等，虽然JVM规范将方法区描述为堆的一个逻辑部分，但它却还有一个别名叫做Non-Heap(非堆)，目的就是要和堆分开。

​	对于HotSpot虚拟机，很多开发者习惯将方法区称之为“永久代（Parmanent Gen）“，但严格本质上说两者不同，或者说使用永久代来实现方法区而已，永久代是方法区(相当于是一个接口)的一个实现，jdk1.7 已经将原本放在永久代的字符串常量池移走。

​	常量池（Constant Pool ）是方法区的一部分，Class文件除了有类的版本、字段、方法、接口等描述信息外，还有一项信息就是常量池，这部分内容将在类加载后进入方法区的运行时常量池中存放。

### Java7的堆内存

![image-20190224212509953](/Users/denny/Library/Application Support/typora-user-images/image-20190224212509953.png)

### Java8的堆内存

![image-20190224212824762](/Users/denny/Library/Application Support/typora-user-images/image-20190224212824762.png)



### 对象创建

![image-20190224215109081](/Users/denny/Library/Application Support/typora-user-images/image-20190224215109081.png)

### 对象结构

![image-20190224215221661](/Users/denny/Library/Application Support/typora-user-images/image-20190224215221661.png)

### 对象头

![image-20190224215305797](/Users/denny/Library/Application Support/typora-user-images/image-20190224215305797.png)

### 垃圾回收

![image-20190224215403772](/Users/denny/Library/Application Support/typora-user-images/image-20190224215403772.png)

### 引用计数法

比较古老的回收算法。原理是此对象有一个引用，即增加一个计数，删除一个引用则减少一个计数。垃圾回收时，只用收集计数为0的对象。此算法最致命的是无法处理循环引用的问题。

### 可达性分析法

### 标记清除算法

此算法执行分两阶段。第一阶段从引用根节点开始标记所有被引用的对象，第二阶段遍历整个堆，把未标记的对象清除。此算法需要暂停整个应用，同时，会产生内存碎片。

![img](http://dl.iteye.com/upload/picture/pic/49637/df51497d-3678-3eaa-b68b-ad4b8a895f8a.png)

### 复制算法

此算法把内存空间划为两个相等的区域，每次只使用其中一个区域。垃圾回收时，遍历当前使用区域，把正在使用中的对象复制到另外一个区域中。次算法每次只处理正在使用中的对象，因此复制成本比较小，同时复制过去以后还能进行相应的内存整理，不会出现“碎片”问题。当然，此算法的缺点也是很明显的，就是需要两倍内存空间。

![img](http://dl.iteye.com/upload/picture/pic/49639/cdd58f10-c06b-3ab4-b927-5be187773a87.png)

### 标记-整理算法

此算法结合了“标记-清除”和“复制”两个算法的优点。也是分两阶段，第一阶段从根节点开始标记所有被引用对象，第二阶段遍历整个堆，把清除未标记对象并且把存活对象“压缩”到堆的其中一块，按顺序排放。此算法避免了“标记-清除”的碎片问题，同时也避免了“复制”算法的空间问题。

![img](http://dl.iteye.com/upload/picture/pic/49641/784445ce-0966-341e-aaee-d42d081d4113.png)

### 内存分配策略

![image-20190224215658352](/Users/denny/Library/Application Support/typora-user-images/image-20190224215658352.png)

 由于对象进行了分代处理，因此垃圾回收区域、时间也不一样。GC有两种类型：**Scavenge GC**和**Full GC**。

Scavenge GC

​	一般情况下，当新对象生成，并且在Eden申请空间失败时，就会触发Scavenge GC，对Eden区域进行GC，清除非存活对象，并且把尚且存活的对象移动到Survivor区。然后整理Survivor的两个区。这种方式的GC是对年轻代的Eden区进行，不会影响到年老代。因为大部分对象都是从Eden区开始的，同时Eden区不会分配的很大，所以Eden区的GC会频繁进行。因而，一般在这里需要使用速度快、效率高的算法，使Eden去能尽快空闲出来。

Full GC

​	对整个堆进行整理，包括Young、Tenured和Perm。Full GC因为需要对整个对进行回收，所以比Scavenge GC要慢，因此应该尽可能减少Full GC的次数。在对JVM调优的过程中，很大一部分工作就是对于FullGC的调节。有如下原因可能导致Full GC：

```java
· 年老代（Tenured）被写满

· 持久代（Perm）被写满 

· System.gc()被显示调用 

·上一次GC之后Heap的各域分配策略动态变化
```

### 调优方法

* **年老代堆空间被占满**

  **异常：** java.lang.OutOfMemoryError: Java heap space

  **说明：**

  

  ![img](http://dl.iteye.com/upload/picture/pic/51415/49464252-97ea-3ce2-b433-d9088bafb70a.png)

   

  ​    这是最典型的内存泄漏方式，简单说就是所有堆空间都被无法回收的垃圾对象占满，虚拟机无法再在分配新空间。

  ​    如上图所示，这是非常典型的内存泄漏的垃圾回收情况图。所有峰值部分都是一次垃圾回收点，所有谷底部分表示是一次垃圾回收后剩余的内存。连接所有谷底的点，可以发现一条由底到高的线，这说明，随时间的推移，系统的堆空间被不断占满，最终会占满整个堆空间。因此可以初步认为系统内部可能有内存泄漏。（上面的图仅供示例，在实际情况下收集数据的时间需要更长，比如几个小时或者几天）

   

  **解决：**

  ​    这种方式解决起来也比较容易，一般就是根据垃圾回收前后情况对比，同时根据对象引用情况（常见的集合对象引用）分析，基本都可以找到泄漏点。

* **持久代被占满**

  **异常：**java.lang.OutOfMemoryError: PermGen space

  **说明：**

  ​    Perm空间被占满。无法为新的class分配存储空间而引发的异常。这个异常以前是没有的，但是在Java反射大量使用的今天这个异常比较常见了。主要原因就是大量动态反射生成的类不断被加载，最终导致Perm区被占满。

  ​    更可怕的是，不同的classLoader即便使用了相同的类，但是都会对其进行加载，相当于同一个东西，如果有N个classLoader那么他将会被加载N次。因此，某些情况下，这个问题基本视为无解。当然，存在大量classLoader和大量反射类的情况其实也不多。

  **解决：**

  * -XX:MaxPermSize=16m

  * 换用JDK。比如JRocket。

* **堆栈溢出**

  **说明：**这个就不多说了，一般就是递归没返回，或者循环调用造成

  **异常：**java.lang.StackOverflowError

* **线程堆栈满**

  **说明**：java中一个线程的空间大小是有限制的。JDK5.0以后这个值是1M。与这个线程相关的数据将会保存在其中。但是当线程空间满了以后，将会出现上面异常。

  **异常**：Fatal: Stack size too small

* **系统内存被占满**

  **说明**：

  ​    这个异常是由于操作系统没有足够的资源来产生这个线程造成的。系统创建线程时，除了要在Java堆中分配内存外，操作系统本身也需要分配资源来创建线程。因此，当线程数量大到一定程度以后，堆中或许还有空间，但是操作系统分配不出资源来了，就出现这个异常了。

  分配给Java虚拟机的内存愈多，系统剩余的资源就越少，因此，当系统内存固定时，分配给Java虚拟机的内存越多，那么，系统总共能够产生的线程也就越少，两者成反比的关系。同时，可以通过修改-Xss来减少分配给单个线程的空间，也可以增加系统总共内生产的线程数。

  **异常**：java.lang.OutOfMemoryError: unable to create new native thread

  **解决：**

  * 重新设计系统减少线程数量。

  * 线程数量不能减少的情况下，通过-Xss减小单个线程大小。以便能生产更多的线程。

## Nginx

## SpringBoot

## SpringCloud-Eureka

## SpringCloud-Feign

## SpringCloud-Ribbon

## SpringCloud-Hystrix

## SpringCloud-Zuul

## 并发编程

## Spring源码

## Mybabis源码

## MySQL性能优化







